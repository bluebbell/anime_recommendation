# -*- coding: utf-8 -*-
"""FIX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c-CoFR-KRrZ_Ofw4iOrSqHhYHeJ_xhPF

# Visualisasi Data
"""

# Commented out IPython magic to ensure Python compatibility.
## Database Phase
import pandas as pd
import numpy as np

#Visualization Phase
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
import matplotlib.pylab as pylab
# %matplotlib inline
pd.set_option('display.max_columns', 500)
mpl.style.use('ggplot')
sns.set_style('white')
pylab.rcParams['figure.figsize'] = 12,8

#ignore warnings
import warnings
warnings.filterwarnings('ignore')

#reading the datasets
anime_data=pd.read_csv('anime.csv')
rating_data=pd.read_csv('rating.csv')

## print shape of dataset with rows and columns and information - anime_data
print ("The shape of the  data is (row, column):"+ str(anime_data.shape))
print (anime_data.info())
## print shape of dataset with rows and columns and information- user rating
print ("The shape of the  data is (row, column):"+ str(rating_data.shape))
print (rating_data.info())

anime_data.head(13000)

import pandas as pd

# Read the CSV file into a pandas DataFrame
data = pd.read_csv('anime.csv')

# Commented out IPython magic to ensure Python compatibility.
## Database Phase
import pandas as pd
import numpy as np

#Visualization Phase
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
import matplotlib.pylab as pylab
# %matplotlib inline
pd.set_option('display.max_columns', 500)
mpl.style.use('ggplot')
sns.set_style('white')
pylab.rcParams['figure.figsize'] = 12,8

#ignore warnings
import warnings
warnings.filterwarnings('ignore')

import pandas as pd

# Membaca file CSV menjadi DataFrame
data = pd.read_csv('anime.csv')

# Menampilkan informasi tentang nilai yang hilang dalam setiap kolom
print(data.isnull().sum())

# Mengisi nilai yang hilang dengan nilai rata-rata kolom
data.fillna(data.mean(), inplace=True)

# Atau menghapus baris dengan nilai yang hilang
# data.dropna(inplace=True)

# Menyimpan DataFrame yang telah diubah ke file CSV
data.to_csv('anime.csv', index=False)

#reading the datasets
anime_data=pd.read_csv('anime.csv')
rating_data=pd.read_csv('rating.csv')

## print shape of dataset with rows and columns and information - anime_data
print ("The shape of the  data is (row, column):"+ str(anime_data.shape))
print (anime_data.info())
## print shape of dataset with rows and columns and information- user rating
print ("The shape of the  data is (row, column):"+ str(rating_data.shape))
print (rating_data.info())

"""## Merging Dataframes"""

#We are merging our csv's based on anime_id from both datasets and later we are renaming the columns
anime_fulldata=pd.merge(anime_data,rating_data,on='anime_id',suffixes= ['', '_user'])
anime_fulldata = anime_fulldata.rename(columns={'name': 'anime_title', 'rating_user': 'user_rating'})
anime_fulldata.head()

"""## Genre Word Cloud"""

nonull_anime=anime_fulldata.copy()
nonull_anime.dropna(inplace=True)
from collections import defaultdict

all_genres = defaultdict(int)

for genres in nonull_anime['genre']:
    for genre in genres.split(','):
        all_genres[genre.strip()] += 1

from wordcloud import WordCloud

genres_cloud = WordCloud(width=800, height=400, background_color='white', colormap='gnuplot').generate_from_frequencies(all_genres)
plt.imshow(genres_cloud, interpolation='bilinear')
plt.axis('off')

anime_data[['name', 'rating', 'members', 'type']].sort_values(by='rating', ascending=False).query('members>500000')[:5]

"""## Preparing data for consumption

### Handling NaN values
"""

#Replacing -1 with NaN in user_rating column
anime_feature=anime_fulldata.copy()
anime_feature["rating"].replace({-1: np.nan}, inplace=True)
anime_feature.head()

#dropping all the null values as it aids nothing
anime_feature = anime_feature.dropna(axis = 0, how ='any')
anime_feature.isnull().sum()

"""### Filtering user_id"""

counts = anime_feature['user_id'].value_counts()
anime_feature = anime_feature[anime_feature['user_id'].isin(counts[counts >= 10].index)]

"""### Cosine Similarity"""

anime_pivot=anime_feature.pivot_table(index='anime_title',columns='user_id',values='rating').fillna(0)
anime_pivot.head()

"""# Recommendation building phase

## Collaborative Filtering

### Cosine Similarity using KNN
"""

from scipy.sparse import csr_matrix
#Creating a sparse matrix
anime_matrix = csr_matrix(anime_pivot.values)

from sklearn.neighbors import NearestNeighbors

#Fitting the model
model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')
model_knn.fit(anime_matrix)

from sklearn.feature_extraction.text import TfidfVectorizer

genres_str = anime_data['genre'].str.split(',').astype(str)

tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 4), min_df=0)
tfidf_matrix = tfidf.fit_transform(genres_str)

tfidf_matrix.shape
# tfidf.get_feature_names()

query_index = np.random.choice(anime_pivot.shape[0])
#print(query_index)
distances, indices = model_knn.kneighbors(anime_pivot.iloc[query_index,:].values.reshape(1, -1), n_neighbors = 6)

"""### Testing collaborative recommendation"""

for i in range(0, len(distances.flatten())):
    if i == 0:
        print('Recommendations for {0}:\n'.format(anime_pivot.index[query_index]))
    else:
        print('{0}: {1}, with distance of {2}:'.format(i, anime_pivot.index[indices.flatten()[i]], distances.flatten()[i]))

"""## Content based filtering

### Cleaning anime_title
"""

#Sharingan copy:https://www.kaggle.com/indralin/try-content-based-and-collaborative-filtering
import re
def text_cleaning(text):
    text = re.sub(r'&quot;', '', text)
    text = re.sub(r'.hack//', '', text)
    text = re.sub(r'&#039;', '', text)
    text = re.sub(r'A&#039;s', '', text)
    text = re.sub(r'I&#039;', 'I\'', text)
    text = re.sub(r'&amp;', 'and', text)

    return text

anime_data['name'] = anime_data['name'].apply(text_cleaning)

"""### Term Frequency (TF) and Inverse Document Frequency (IDF)"""

from sklearn.feature_extraction.text import TfidfVectorizer

#getting tfidf
tfv = TfidfVectorizer(min_df=3,  max_features=None,
            strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3),
            stop_words = 'english')

# Filling NaNs with empty string
anime_data['genre'] = anime_data['genre'].fillna('')
genres_str = anime_data['genre'].str.split(',').astype(str)
tfv_matrix = tfv.fit_transform(genres_str)

from sklearn.metrics.pairwise import sigmoid_kernel

# Compute the sigmoid kernel
sig = sigmoid_kernel(tfv_matrix, tfv_matrix)

#getting the indices of anime title
indices = pd.Series(anime_data.index, index=anime_data['name']).drop_duplicates()

"""## Content based Recommendation function"""

def give_rec(title, sig=sig):
    # Get the index corresponding to original_title
    idx = indices[title]

    # Get the pairwsie similarity scores
    sig_scores = list(enumerate(sig[idx]))

    # Sort the movies
    sig_scores = sorted(sig_scores, key=lambda x: x[1], reverse=True)

    # Scores of the 10 most similar movies
    sig_scores = sig_scores[1:11]

    # Movie indices
    anime_indices = [i[0] for i in sig_scores]

    # Top 10 most similar movies
    return pd.DataFrame({'Anime name': anime_data['name'].iloc[anime_indices].values,
                                 'Rating': anime_data['rating'].iloc[anime_indices].values})

give_rec('Cowboy Bebop')

"""# Different Source

## Content-Based Recommendation System
"""

from sklearn.feature_extraction.text import TfidfVectorizer

genres_str = anime_data['genre'].str.split(',').astype(str)

tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 4), min_df=0)
tfidf_matrix = tfidf.fit_transform(genres_str)

tfidf_matrix.shape
# tfidf.get_feature_names()

from sklearn.metrics.pairwise import linear_kernel

cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

indices = pd.Series(anime_data.index, index=anime_data['name'])

def genre_recommendations(title, similarity=False):

    if similarity == False:

        idx = indices[title]
        sim_scores = list(enumerate(cosine_sim[idx]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = sim_scores[1:11]

        anime_indices = [i[0] for i in sim_scores]

        return pd.DataFrame({'Anime name': anime_data['name'].iloc[anime_indices].values,
                             'Type': anime_data['type'].iloc[anime_indices].values})

    elif similarity == True:

        idx = indices[title]
        sim_scores = list(enumerate(cosine_sim[idx]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = sim_scores[1:11]

        anime_indices = [i[0] for i in sim_scores]
        similarity_ = [i[1] for i in sim_scores]

        return pd.DataFrame({'Anime name': anime_data['name'].iloc[anime_indices].values,
                             'similarity': similarity_,
                             'Type': anime_data['type'].iloc[anime_indices].values})

"""### Recommendation based on cosine similarity"""

indices = pd.Series(anime_data.index, index=anime_data['name'])

def genre_recommendations(title, highest_rating=False, similarity=False):

    if highest_rating == False:
        if similarity == False:

            idx = indices[title]
            sim_scores = list(enumerate(cosine_sim[idx]))
            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
            sim_scores = sim_scores[1:11]

            anime_indices = [i[0] for i in sim_scores]

            return pd.DataFrame({'Anime name': anime_data['name'].iloc[anime_indices].values,
                                 'Type': anime_data['type'].iloc[anime_indices].values})

        elif similarity == True:

            idx = indices[title]
            sim_scores = list(enumerate(cosine_sim[idx]))
            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
            sim_scores = sim_scores[1:11]

            anime_indices = [i[0] for i in sim_scores]
            similarity_ = [i[1] for i in sim_scores]

            return pd.DataFrame({'Anime name': anime_data['name'].iloc[anime_indices].values,
                                 'Similarity': similarity_,
                                 'Type': anime_data['type'].iloc[anime_indices].values})

    elif highest_rating == True:
        if similarity == False:

            idx = indices[title]
            sim_scores = list(enumerate(cosine_sim[idx]))
            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
            sim_scores = sim_scores[1:11]

            anime_indices = [i[0] for i in sim_scores]

            result_df = pd.DataFrame({'Anime name': anime_data['name'].iloc[anime_indices].values,
                                 'Type': anime_data['type'].iloc[anime_indices].values,
                                 'Rating': anime_data['rating'].iloc[anime_indices].values})

            return result_df.sort_values('Rating', ascending=False)

        elif similarity == True:

            idx = indices[title]
            sim_scores = list(enumerate(cosine_sim[idx]))
            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
            sim_scores = sim_scores[1:11]

            anime_indices = [i[0] for i in sim_scores]
            similarity_ = [i[1] for i in sim_scores]

            result_df = pd.DataFrame({'Anime name': anime_data['name'].iloc[anime_indices].values,
                                 'Similarity': similarity_,
                                 'Type': anime_data['type'].iloc[anime_indices].values,
                                 'Rating': anime_data['rating'].iloc[anime_indices].values})

            return result_df.sort_values('Rating', ascending=False)

genre_recommendations('Haikyuu!!', highest_rating=True, similarity=True)

genre_recommendations('Cowboy Bebop', highest_rating=False, similarity=False)

pip install pandas scikit-learn

import pandas as pd

anime_df = pd.read_csv("anime.csv")
rating_df = pd.read_csv("rating.csv")

merged_df = pd.merge(rating_df, anime_df, on="anime_id")

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(merged_df, test_size=0.2, random_state=42)

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Contoh: Menggunakan rata-rata rating sebagai rekomendasi
predicted_ratings = np.full(test_df.shape[0], train_df["rating"].mean())
mae = mean_absolute_error(test_df["rating"], predicted_ratings)
rmse = np.sqrt(mean_squared_error(test_df["rating"], predicted_ratings))

print(f"Mean Absolute Error: {mae}")
print(f"Root Mean Squared Error: {rmse}")